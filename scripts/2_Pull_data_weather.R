#----------------------------------------------------------------------------------------------------------------------------------#
# Script by : Lucien Fitzpatrick
# Project: Collection phenology vulnerability
# Purpose: This script serves as the download of weather data and the calculation of relevant weather statistics
# Inputs: The weather_calc.r script which defines the weather_calc function.
# Outputs: lat.calc dataframe that can be used to match weather to phenological events
# Notes:
#-----------------------------------------------------------------------------------------------------------------------------------#

#-------------------------------------------------#
#This section is for downloaded the met data, pulling out data of interest, and calculating growing degree days
#-------------------------------------------------#

path.g <- "G:/My Drive"
path.hub <- "C:/Users/lucie/Documents/GitHub/Collections_phenology_vulnerability"
# path.g <- "/Volumes/GoogleDrive/My Drive"
# path.hub <- ".."


path.daymet <- "../data_raw/DAYMET"
if(!dir.exists(path.daymet)) dir.create(path.daymet, recursive = T)

dat.burst <- read.csv(paste0(path.hub, "/data/Oak_collection_budburst_raw.csv"))
dat.leaf <- read.csv(paste0(path.hub, "/data/Oak_collection_leaf_raw.csv"))
dat.comb <- rbind(dat.burst, dat.leaf)


# Note: This will probably change down the road
NPN.pts <- aggregate(Year~Site+Latitude+Longitude, data=dat.burst, 
                     FUN=min)
names(NPN.pts)[4] <- "yr.start"
NPN.pts$yr.end <- aggregate(Year~Site+Latitude+Longitude, data=dat.burst, 
                            FUN=max)[,4]
NPN.pts


#Writing the csv file of lat and longs because daymetr batch function needs to read a file instead of a dataframe
write.csv(NPN.pts, file.path(path.daymet, "NPN_points.csv"), row.names=FALSE)

# if(!dir.exist(path.daymet)) dir.create(path.daymet)
#Downloading all of the damet data for each point. Internal =TRUE means it creates a nested list. Set false to actually download a file
lat.list <- daymetr::download_daymet_batch(file_location = file.path(path.daymet, "NPN_points.csv"),
                                           start = min(NPN.pts$yr.start),
                                           end = max(NPN.pts$yr.end),
                                           internal = T)


#removing failed downloads 
lat.list <- lat.list[sapply(lat.list, function(x) is.list(x))]


# This gives us a list with one layer per site (I think)
length(lat.list)
names(lat.list) <- NPN.pts$site # Giving the different layers of the list the site names they correspond to

#Lets look at the structure of what we are given
summary(lat.list)

list.met <- list()
for(i in seq_along(lat.list)){
  list.met[[i]] <- data.frame(site=NPN.pts$Site[i], latitude=NPN.pts$Latitude[i], longitude=NPN.pts$Longitude[i], lat.list[[i]]$data)
}
names(list.met) <-  NPN.pts$site

rm(lat.list) # Removing lat.list to save memory


#Reading in our function for calculating weather statistics of interest
source(file.path(path.hub, "scripts/Weather_calc.R"))

#Running our function to calculate weather statistics. Default year range is 1975-2019. Growing seaosn is yday 1 to 120

list.met<- lapply(list.met, weather_calc)

lat.calc <- dplyr::bind_rows(list.met)

dir.create("../data_processed/Arb_Pheno/", recursive=T, showWarnings = F)
write.csv(lat.calc, "../data_processed/Arb_Daymet_clean_data.csv", row.names=F)


