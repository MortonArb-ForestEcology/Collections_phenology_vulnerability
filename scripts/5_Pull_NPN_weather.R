#----------------------------------------------------------------------------------------------------------------------------------#
# Script by: Lucien Fitzpatrick
# Project: Collection phenology vulnerability
# Purpose: This script downloads the neccessary daymet weather data to match our NPN sites
# Inputs: NPN data for relevant species as downloaded in script 4
# Outputs: Data frame containing all weather data needed for npn sites
# Notes: This script takes nearly an hour to run because of the size of the data
#-----------------------------------------------------------------------------------------------------------------------------------#

#Reading in our bud burst and leaf out data frames
dat.leaf <- read.csv("../data_raw/Raw_Phenology_NPN_combined_leaf.csv")

dat.budburst <- read.csv("../data_raw/Raw_Phenology_NPN_combined_budburst.csv")


path.daymet <- "../data_raw/DAYMET"

#Creating combined data frame so that we can donwload from all sites with one go considering the amount of overlap
dat.sites <- rbind(dat.leaf, dat.budburst)

NPN.pts <- aggregate(year~site_id+latitude+longitude, data=dat.sites, 
                     FUN=min)
names(NPN.pts)[4] <- "yr.start"
NPN.pts$yr.end <- aggregate(year~site_id+latitude+longitude, data=dat.sites, 
                            FUN=max)[,4]
NPN.pts


#Writing the csv file of lat and longs because daymetr batch function needs to read a file instead of a dataframe
write.csv(NPN.pts, file.path(path.daymet, "NPN_points.csv"), row.names=FALSE)


#Downloading all of the damet data for each point. Internal =TRUE means it creates a nested list. Set false to actually download a file
#At its current size (991 sites) +.this can take about an hour to run
lat.list <- daymetr::download_daymet_batch(file_location = file.path(path.daymet, "NPN_points.csv"),
                                           start = 1980,
                                           end = 2019,
                                           internal = T)


#removing failed downloads 
lat.list <- lat.list[sapply(lat.list, function(x) is.list(x))]


# This gives us a list with one layer per site (I think)
length(lat.list)
names(lat.list) <- NPN.pts$site # Giving the different layers of the list the site names they correspond to

#Lets look at the structure of what we are given
summary(lat.list)

list.met <- list()
for(i in seq_along(lat.list)){
  list.met[[i]] <- data.frame(site=NPN.pts$site_id[i], latitude=NPN.pts$latitude[i], longitude=NPN.pts$longitude[i], lat.list[[i]]$data)
}
names(list.met) <-  NPN.pts$site

rm(lat.list) # Removing lat.list to save memory
#Reading in our function for calculating weather statistics of interest
source(file.path("weather_calc.R"))

#Running our function to calculate weather statistics. Default year range is 1975-2019. Growing seaosn is yday 1 to 120
list.met<- lapply(list.met, weather_calc)

lat.calc <- dplyr::bind_rows(list.met)

write.csv(lat.calc, "../data_processed/Daymet_clean_data.csv", row.names=F)
